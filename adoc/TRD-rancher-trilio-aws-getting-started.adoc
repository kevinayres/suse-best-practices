:docinfo:

// = {title}
= TrilioVault with SUSE Rancher on AWS - Getting Started

// SUSE Rancher - Trilio - AWS
// :author: Kevin Ayres
:revnumber: 0.0.1
:toc2:
:toc-title: TrilioVault with SUSE Rancher on AWS - Getting Started

:toclevels: 4

:sles: SUSE Linux Enterprise Server

== Motivation
Agility is the name of the game in modern application development.  This is driving developers toward more agile, 
cloud native methodologies that focus on microservices architectures and streamlined workflows.  
Container technologies, like Kubernetes, embody this agile approach and help enable cloud native transformation.

SUSE Rancher simplifies Kubernetes management, empowering you to take control of your IT landscape and create 
an agile data platform that accelerates achievement of your goals.  
Rancher enables you to manage dynamic, robust, multi-cluster Kubernetes environments and supports any 
CNCF-certified Kubernetes distribution.  With built-in resilience and scalability, unified security and policy management, 
and a rich catalog of shared tools and services, Rancher helps you accelerate development-to-production and innovate everywhere.

Amazon Web Services the world leader in public cloud services. Natice storage services within AWS, provide an ideal integration point for SUSE Rancher with TrilioVault backups. By leveraging the immutable storage capabilities within TrilioVault and Amazon S3 and Glacier, the administrator is enabled toward unparalleled data integrity and mobility. 

TrilioVault is a cloud native backup technology that brings all the power of the traditional, 
backup software and provides a more natural way to work with modern data.  
TrilioVault offers a variety of compelling features that make it a natural partner for a SUSE Rancher agile data platform.  
These include:

* Data Protection:
Based on Trilio's backup and recovery technology, point-in-time backups and restores can be created for cloud-native applications protecting them from data corruption or other malicious activity on production data.

* Disaster Recovery:
Based on Trilio's backup and recovery technology, point-in-time backups and restores can be created for cloud-native applications protecting them from data corruption or other malicious activity on production data. The same backup and recovery technology can be leveraged in case of outages at the primary site to restore an entire environment to a separate location, fulfilling Disaster Recovery.
TrilioVault backups are application aware and includes all artifacts that define the application. These include persistent data, Pod definitions, config maps, secrets, and other items. It recovers the entire application allowing you to achieve the most optimal RTO possible. 

* Migration:
You may need to migrate applications from one cloud platform to another or one Kubernetes cluster to a different Kubernetes cluster. Most businesses and compliance initiatives are now mandating IT organizations to have a proven multi-cloud strategy - so that they are not locked to a specific cloud vendor. IT departments are now required to demonstrate their multi-cloud strategy by recovering business applications in multiple clouds. By choosing cloud storage as backup target, TrilioVault can demonstrate an application migration use case.

* Test and Development:
Another popular use case is test and development for DevOps. There are a plethora of tools that can help you achieve DevOps, but most of them lack the capability to version control the production data. Exiting DevOp tools can help version control the code and help you streamline processes. However, in some instances you may a need point-in-time of production data for you DevOps processes including identifying performance bottlenecks, troubleshooting data corruption and other issues. TrilioVault can reliably restore a point-in-time of your production application including its associated data for test and development needs.

* Ransomware Protection:
Trilio provides and will continue to innovate its strategy to protect against ransomware attacks. Ransomware protection from Trilio is aligned to the pillars of the NIST and NCCoE cybersecurity frameworks. The pillars are defined as Identify and Protect, Detect and Mitigate, and Recover. Trilio has built (and is building) features to align with these pillars:

. Identify and Protect - Application discovery, Security Validations Immutable backups, Encryption, Zero-Trust etc.
. Detect and Mitigate - Scanning of Backups, Anomaly Detection, Notifications into Slack/Teams etc.
. Recover - Deep logging, Isolation testing, DR workflows and multiple target types to increase recoverability surface.


== Technical overview

SUSE Rancher is a lightweight Kubernetes installer that supports installation on bare-metal and virtualized machines and instances.  
Rancher solves a common issue in the Kubernetes community: installation complexity.  With Rancher, Kubernetes installation is greatly simplified.

This document reviews considerations for deploying and managing a highly available, MongoDB NoSQL database on a 
SUSE Rancher Kubernetes cluster.

In practice, the process is as follows:

* Install a Kubernetes cluster through Rancher Kubernetes Engine
* Install a cloud native storage solution on Kubernetes
* Deploy https://docs.mongodb.com/kubernetes-operator/master/[MongoDB Enterprise Kubernetes Operator]
* Configure a storage class and define storage requirements via Operator
* Test failover by killing or cordoning nodes in your cluster


== Value of HA for data

One of the primary benefits of running a Kubernetes environment is flexibility, the ability to easily adapt to varying circumstances.  
Traditional database deployments exist in fairly static configurations and environments.  The beauty of running a data-oriented service 
on Kubernetes lies in maintaining stability while enabling adaptability to meet real-world situations.

Imagine a scenario where your e-commerce site is consistently taking 100 orders per day.  Suddenly, a viral marketing event occurs, 
and your site is pushed to 5000 orders per day for a day.  This increase could easily lead to data overload – or worse, 
corruption or downtime, which could result to considerable loss of revenue.  Having a way to design for such failure scenarios 
and maintain resilient operations is a tangible market advantage.

MongoDB can run in a single node configuration and in a clustered configuration using replica sets (not to be confused with 
Kubernetes Stateful Sets). A replica set is a group of MongoDB instances that maintain the same data. A replica set contains 
several data-bearing nodes and optionally one arbiter node. Of the data-bearing nodes, one and only one member is deemed the primary node,
while the other nodes are deemed secondary nodes.  Resiliency of the data is achieved, as illustrated below.

image::rancher-mongo-1.png[scaledwidth="75%", align="center"]

In this configuration, the failover process generally completes within a minute.  
It may take about 30 seconds for the members of a replica set to declare a primary inaccessible. 
One of the remaining secondaries will then be enabled as the "new primary".  The election itself may take another 10 to 30 seconds.  
During this time, the data will be preserved in a virtually seamless way for dependent services.


== Setting up a cluster with SUSE Rancher

SUSE Rancher is a tool to install and configure Kubernetes in a choice of environments including bare metal, virtual machines, and IaaS. 
Rancher is a complete container management platform built on upstream Kubernetes.
It consists of three major components:

* A certified Kubernetes Distribution – EKS
* A Kubernetes Management platform (Rancher)
* Application Catalog and management (Third-party)

Rancher has the capabilities to manage any Kubernetes cluster from a central location, via the
Rancher server.  As illustrated below, Rancher can manage any Kubernetes flavor such as EKS, and is not restricted to RKE or K3S.

image::rancher-mongo-2.png[scaledwidth="75%", align="center"]

For reference, see Rancher deployment QuickStart guide for details on installation. https://github.com/rancher/quickstart    
By the end of this step, you should have two clusters with multiple master and worker nodes. 

image::rancher-mongo-3.png[scaledwidth="75%", align="center"]


== Storage considerations

When deploying an application that needs to retain data, you need to create persistent storage. 
Persistent storage allows you to store application data external from the pod running your application. 
This storage practice allows you to maintain application data, even if the application’s pod fails.

A variety of storage options exist and can be used to create an HA data solution with Rancher.  
Some considerations you may need to follow for your storage solution include:

* Volumes as persistent storage for the distributed stateful applications

* Partitioned block storage for Kubernetes volumes with or without a cloud provider

* Replicated block storage across multiple nodes and data centers to increase availability

* Secondary data backup storage (for example, NFS or S3)

* Cross-cluster disaster recovery volumes

* Recurring volume snapshots

* Recurring backups to secondary storage

* Non-disruptive upgrades



Some common storage solutions to consider are as follows:

https://longhorn.io[Longhorn]:: Distributed block storage system for Kubernetes. Originally developed by Rancher Labs.  
Currently sandbox project of the Cloud Native Computing Foundation

https://openebs.io[OpenEBS]:: Open source, CNCF Sandbox storage with flexible storage engine options - requires third-party integration

https://ceph.io[Ceph]:: Powerful, open source, general purpose storage in the CNCF Sandbox – requires third-party integration

https://portworx.com[Portworx]:: Proprietary solution with Rancher certified integration - installation steps can be found https://docs.portworx.com/install-with-other/rancher/rancher-2.x[here]



=== Setting up your storage

Before proceeding, be sure that you understand the Kubernetes concepts of persistent volumes, persistent volume claims, and 
storage classes.  For more information, refer to 
https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works[How Persistent Storage Works] in the Rancher documentation.

The workflow for setting up existing storage is as follows:

. Ensure you have access to set up your persistent storage. This may be storage in an infrastructure provider, or it could be your own storage.

. Add a persistent volume (PV) that refers to the persistent storage.

. Add a persistent volume claim (PVC) that refers to the PV.

. Mount the PVC as a volume in your workload.

For further details and prerequisites, read the Rancher documentation section 
https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage[Setting Up Existing Storage].


The overall workflow for provisioning new storage is as follows:

1. Add a StorageClass and configure it to use your storage provider. The StorageClass could refer to storage in an infrastructure provider, or it could refer to your own storage.
2. Add a persistent volume claim (PVC) that refers to the storage class.
3. Mount the PVC as a volume for your workload.

See section https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage[Dynamically Provisioning New Storage in Rancher] for details and prerequisites.


=== Creating a storage class for MongoDB

When the Kubernetes cluster is running and storage is configured, it is time to deploy a highly available MongoDB database.

MongoDB resources are created in Kubernetes as custom resources. After you create or update a MongoDB Kubernetes resource specification, 
you direct MongoDB Kubernetes Operator to apply this specification to your Kubernetes environment. 
Kubernetes Operator creates the defined StatefulSets, services and other Kubernetes resources. 
After the Operator finishes creating those objects, it updates the Ops Manager deployment configuration to reflect changes.

The following example shows a resource specification for a 
https://docs.mongodb.com/manual/reference/glossary/#term-replica-set[replica set] configuration:

[source,bash]
----
apiVersion: mongodb.com/v1
kind: MongoDB
metadata:
  name: my-replica-set
spec:
  members: 3
  version: "4.2.2-ent"
  service: my-service
  opsManager: # Alias of cloudManager
    configMapRef:
      name: my-project
  credentials: my-credentials
  persistent: true
  type: ReplicaSet
  podSpec:
    cpu: "0.25"
    memory: "512M"
    persistence:
      multiple:
        data:
          storage: "10Gi"
        journal:
          storage: "1Gi"
          labelSelector:
            matchLabels:
              app: "my-app"
        logs:
          storage: "500M"
          storageClass: standard
    podAntiAffinityTopologyKey: nodeId
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
    podTemplate:
      metadata:
        labels:
          label1: mycustomlabel
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  topologyKey: "mykey"
                weight: 50
  security:
    tls:
      enabled: true
    authentication:
      enabled: true
      modes: ["X509"]
      internalCluster: "X509"
  additionalMongodConfig:
    net:
      ssl:
        mode: preferSSL
----

Full details can be found https://docs.mongodb.com/kubernetes-operator/master/reference/k8s-operator-specification[here].


== Creating a persistent volume

You can now create a persistent volume claim (PVC) based on the storage class. Dynamic provisioning will be created without explicitly 
provisioning a persistent volume (PV). As part of deployment, the Kubernetes operator creates 
https://kubernetes.io/docs/concepts/storage/persistent-volumes[persistent volumes] for the Ops Manager StatefulSets. 
The Kubernetes container uses persistent volumes to maintain the cluster state between restarts.

== TrilioVault for Kubernetes Pre-Flight

Before proceeding with the install of the TrilioVault for Kubernetes solution, a pre-flight check should be performed to ensure that the cluster is prepared correctly for a TrilioVault installation. Trilio provides a preflight check via Krew (kubectl based extension) to validate a Kubernetes cluster before install. The preflight validator confirms configuration settings such as availability of a storageclass, volumesnapshotclass, correct utilities exist etc. 
Please follow the instructions from https://docs.trilio.io/kubernetes/support/support-and-issue-filing/tvk-preflight-checks[Trilio Pre-Flight]

== Deploying TrilioVault for Kubernetes Operator

TrilioVault for Kubernetes provides an operator as a Rancher Partner Chart for Kubernetes cluster deployments and it is present on the Rancher Apps & Marketplace. Here are the instructions to install TVK as a Rancher Partner Chart on RKE cluster deployment.

Note: TVK Operator deployment as Rancher Partner chart is supported only on Rancher server v2.5 and above.


. Login to the Rancher Server web console
. Select the RKE cluster deployment from the list of cluster deployments, click on the cluster name to view details
. Click on the Cluster Explorer on top right corner to view all resources
. Click on the dropdown 'Cluster Explorer' on the top left corner and select 'Apps & Marketplace'
. From the list of the Partner Charts type 'Trilio' in the filter to see TVK operator
. Click on the k8s-triliovault-operator  to begin the Operator installation on the K8s cluster
. You can select the desired namespace from the 'Namespace' dropdown list for the installation. 
. Click on 'Install' button to start the installation. Users can follow the installation progress in terminal window

[source,bash]
----
helm install --namespace=default --timeout=10m0s --values=/home/shell/helm/values-k8s-triliovault-operator-v2.0.200.yaml --version=v2.0.200 --wait=true k8s-triliovault-operator /home/shell/helm/k8s-triliovault-operator-v2.0.200.tgz
creating 8 resource(s)
beginning wait for 8 resources with timeout of 10m0s
Deployment is not ready: default/k8s-triliovault-operator. 0 out of 1 expected pods are ready
Deployment is not ready: default/k8s-triliovault-operator. 0 out of 1 expected pods are ready
Deployment is not ready: default/k8s-triliovault-operator. 0 out of 1 expected pods are ready
Deployment is not ready: default/k8s-triliovault-operator. 0 out of 1 expected pods are ready
NAME: k8s-triliovault-operator
LAST DEPLOYED: Thu Apr 8 07:15:53 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
To verify that TrilioVault Operator has started, run:
kubectl --namespace=default get deployments -l "release=k8s-triliovault-operator"
---------------------------------------------------------------------
SUCCESS: helm install --namespace=default --timeout=10m0s --values=/home/shell/helm/values-k8s-triliovault-operator-v2.0.200.yaml --version=v2.0.200 --wait=true k8s-triliovault-operator /home/shell/helm/k8s-triliovault-operator-v2.0.200.tgz
---------------------------------------------------------------------
----

After the installation is complete, you can verify it from the list of 'Installed Apps'


== Installing TrilioVault Manager

TrilioVault manager is an instance of the TrilioVault solution running in your namespace or your cluster (as the solution supports both namespace and cluster scoped deployments)

This section assumes that you have installed kubectl and helm installed and correctly configured to work with desired Kubernetes cluster. 
As part of the install, Trilio first requires the Operator to be deployed (which was done in the previous step), after which the application is deployed via the TrilioVault Manager (TVM) Resource created by the Operator.

* Add the Trilio Helm repository to your local setup
[source,bash]
----
helm repo add triliovault-operator http://charts.k8strilio.net/trilio-stable/k8s-triliovault-operator
helm repo add triliovault http://charts.k8strilio.net/trilio-stable/k8s-triliovault
helm repo update
----

* Verify TrilioVault operator pods are running
[source,bash]
----
$ kubectl get pods -l release=triliovault-operator
NAME                                                             READY   STATUS    RESTARTS   AGE
triliovault-operator-k8s-triliovault-operator-7bf447967f-w5tgd   1/1     Running   0          2m7s
----

* Deploy TrilioVault Manager
The TrilioVault custom resource name is TrilioVaultManager
TrilioVault Operator defines this Custom Resource (CR).
To configure resource limits for TVK components please refer to https://docs.trilio.io/kubernetes/performance/configuring-resource-limits [Resource Limits]
Installation via Helm v3
[source,bash]
----
apiVersion: triliovault.trilio.io/v1
kind: TrilioVaultManager
metadata:
  labels:
    triliovault: triliovault
  name: triliovault-manager
  namespace: default
spec:
  trilioVaultAppVersion: v2.1.0
  helmVersion:
    version: v3
  applicationScope: Cluster
  #restoreNamespaces: ["kube-system", "default", "restore-ns"]
  #resources:
    #requests:
      #memory: 400Mi
----

In the CRD example below,restoreNamespaces is optional.  To restrict restores to specific namespaces  specify the namespaces for that field.
[source,bash]
----
$ kubectl create -f triliovault-manager.yaml
----

* List CR of TrilioVaultManager
[source,bash]
----
kubectl get triliovaultmanager
NAME                  TRILIOVAULT-VERSION   SCOPE     STATUS     RESTORE-NAMESPACES
triliovault-manager   v2.1.0                Cluster   Deployed   [kube-system default restore-ns]
----

* List pods created by TrilioVaultManager CR are running
[source,bash]
----
$ kubectl get pods
k8s-triliovault-admission-webhook-544b566979-4lw7q                1/1     Running     0          7d2h
k8s-triliovault-backend-5b79996f48-djzd4                          1/1     Running     0          7d2h
k8s-triliovault-control-plane-78c7d589fb-d2829                    1/1     Running     0          7d2h
k8s-triliovault-exporter-789c785968-vn7hf                         1/1     Running     0          7d2h
k8s-triliovault-ingress-controller-54c55b58cf-vw7s7               1/1     Running     0          7d2h
k8s-triliovault-web-85d58df67b-jqnln                              1/1     Running     0          7d2h
----

* TrilioVault is now successfully installed on your cluster.

== Test Namespace Capture and Recovery
 
Trilio supports multiple ways of capturing applications - labels, helm, operators, namespaces and direct object reference, however, for this guide we will validate a successful installation and operation of TrilioVault for Kubernetes, perform a Namespace backup/restore.

High-level Steps:

. Create a namespace called 'wordpress'  
. Use helm to deploy a wordpress application into the namespace.
. Perform a backup of the namespace
. Delete the namespace/application
. Create a new namespace 'wordpress-restore'
. Perform a Restore of the namespace

Detailed Steps:

* Create a namespace and deploy the application*
** Create the namespace called 'wordpress'
[source,bash]
----
$ kubectl create ns wordpress
namespace/wordpress created
----

** Install the wordpress Helm Chart
[source,bash]
----
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install my-release bitnami/wordpress
----
You can launch the wordpress app via a browser and make changes to the sample page to ensure changes are captured when you restore.

* Create a Namespace Backup*

** Create a backupPlan to backup the namespace
[source,bash]
----
apiVersion: triliovault.trilio.io/v1
kind: BackupPlan
metadata:
  name: ns-backupplan-1
  namespace: wordpress
spec:
  backupConfig:
    target:
      namespace: default
      name: demo-s3-target
----

** Backup the Namespace
Use the following YAML to build the Backup CR
[source,bash]
----
apiVersion: triliovault.trilio.io/v1
kind: Backup
metadata:
  name: wordpress-ns-backup-1
  namespace: wordpress
spec:
  type: Full
  scheduleType: OneTime
  backupPlan:
    name: ns-backupplan-1
    namespace: wordpress
----

* Delete the namespace*
[source,bash]
----
kubectl delete ns wordpress
----

* Create a new namespace*
[source,bash]
----
kubectl create ns wordpress-restore
----

* Restore the Backup/Namespace*
[source,bash]
----
apiVersion: triliovault.trilio.io/v1
kind: Restore
metadata:
  name: ns-restore
  namespace: wordpress-restore
spec:
  source:
    type: Backup
    backup:
      name: wordpress-ns-backup-1
      namespace: wordpress
  restoreNamespace: wordpress-restore
----

Validate the pods are up and running after restore completes.
[source,bash]
----
$ kubectl get restore
NAME         BACKUP                  STATUS      DATA SIZE   START TIME             END TIME               PERCENTAGE COMPLETED
ns-restore   wordpress-ns-backup-1   Completed   188312911   2020-11-13T18:47:33Z   2020-11-13T18:49:58Z   100
----

[source,bash]
----
$ kubectl get pods 
NAME                              READY   STATUS    RESTARTS   AGE
wordy-mariadb-0                   1/1     Running   0          4m21s
wordy-wordpress-5cc764564-mngrm   1/1     Running   0          4m21s
----

Finally, confirm the changes on the wordpress launch pages that were made earlier.

== Summary

TrilioVault for Kubernetes provides data management and data protection capabilities for your Kubernetes based applications running on SUSE Rancher. Trilio can be managed and operated fully via the CLI as well as the feature rich management console provided by Trilio.

== Additional resources

For more information, visit: 

*	https://rancher.com/docs/rancher/v2.x/en/best-practices/[Rancher best practices guide]
*	https://rancher.com/docs/rancher/v2.x/en/troubleshooting/[Rancher troubleshooting tips]
*	https://docs.trilio.io/kubernetes/[TrilioVault for Kubernetes Documentation]
*	https://docs.trilio.io/kubernetes/use-triliovault/installing-triliovault#rancher-deployments[Trilio Documentation for Rancher Deployments]
*	https://docs.trilio.io/kubernetes/overview/getting-started [TrilioVault for Kubernetes Getting-Started]
*	https://docs.trilio.io/kubernetes/architecture/custom-resource-definitions-application-1 [TrilioVault API Reference]

++++
<?pdfpagebreak?>
++++

:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// include::common_gfdl1.2_i.adoc[]

:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
